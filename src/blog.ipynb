{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this post, we’ll explore how to implement each component of Llama 3 entirely in pure JAX. I grew tired of writing the code in torch, so I decided to switch to JAX—a library that works much like NumPy but offers powerful features such as `AutoDiff`, `jit` and `vmap` which are very helpful for training neural networks.\n",
    "\n",
    "I began this project a few months ago and hit some errors along the way that forced me to put it on hold. Recently, I came across on this [blog post](https://alessiodevoto.github.io/ViT-in-pure-JAX/#the-model-is-just-a-function) where the author implemented a Vision Transformer in pure JAX. That post motivate me, and I finally completed my project. In this blog, I share my journey and detail how I built Llama 3 using pure JAX.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The above figure is of llama architetue. we will bw coverin each part from above image like :\n",
    "\n",
    "- implemting complete model in jax \n",
    "- training it on shakespaere data with 125M size model \n",
    "- Ineference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization \n",
    "\n",
    "- When we canont raw text to the model directly cause model don't undxerstnad our languase so we need to convert it into numbers. \n",
    "Tokenization is process in which we divide the text into words and subwords knows as token. But this is not effiient it requires more memory so there some advanced \n",
    "Tokenization method that we going to use like BPE(this was used in trianing llama3). I will not cover this you can checkout this great explantion of bpe(byte pair encoding) by andrej karpathy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: From fairest creatures we desire increase,\n",
      "Encoded Tokens: [220, 3574, 37063, 301, 8109, 356, 6227, 2620, 11, 198]\n",
      "Decoded Text:   From fairest creatures we desire increase,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import tiktoken\n",
    "\n",
    "# Load GPT-2 BPE encoding\n",
    "enc = tiktoken.get_encoding(\"gpt2\") \n",
    "\n",
    "# reading a line from \n",
    "with open('../shakespeare.txt', 'r') as f:\n",
    "    text = f.readlines()[0]  # Take the first line\n",
    "\n",
    "# Encode the text into token IDs\n",
    "tokens = enc.encode(text)\n",
    "data = jnp.array(tokens, dtype=jnp.int32)  # Store as JAX array\n",
    "\n",
    "# Decode back to text\n",
    "decoded_text = enc.decode(tokens)\n",
    "\n",
    "\n",
    "print(\"original Text:\", text.strip())\n",
    "print(\"encoded Tokens:\", tokens)\n",
    "print(\"decoded Text:\", decoded_text)\n",
    "\n",
    "## Ouput ##\n",
    "\n",
    "# Original Text: From fairest creatures we desire increase,\n",
    "# Encoded Tokens: [220, 3574, 37063, 301, 8109, 356, 6227, 2620, 11, 198]\n",
    "# Decoded Text:   From fairest creatures we desire increase,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings \n",
    "\n",
    "- We need cause we cannot provide the tokens directly cause tokens are descreate but neural networks operate on continuous numerical data. This also helps in mathematical operations (addition, multiplication, etc.). Embedding also help in encoding semantic and syntactic relationships between tokens.Similar tokens (e.g., \"cat\" and \"dog\") are mapped to nearby points in the embedding space. Relationships like analogies (e.g., \"king\" – \"man\" + \"woman\" ≈ \"queen\") can emerge in the vector space. The model learns these patterns during training, enabling it to generalize to unseen text.\n",
    "\n",
    "- LLM use the dynamic embeding which can be updated. Initial embeddings represent the \"identity\" of a token (e.g., the word \"bank\"). Subsequent layers (attention mechanisms) refine these embeddings to incorporate context. For example:\n",
    "\"Bank\" in \"river bank\" vs. \"bank account\" gets different contextualized representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
