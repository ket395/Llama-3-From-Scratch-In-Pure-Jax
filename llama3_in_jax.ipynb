{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing LLaMA3 in 100 Lines of Pure Jax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![img](images/newllama.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eHchWcayWRWP"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Root Mean Square Layer Normalization\n",
        "\n",
        "RMS normalization is an important layer in llama3 models. It helps keep the training stable by making sure that the numbers in the network don’t become too high or too low. This balance is very important, especially in deep networks.\n",
        "\n",
        "![img](images/rsmnorm.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03j4HzyiWT8e"
      },
      "outputs": [],
      "source": [
        "def rms_norm(x, weight, eps=1e-5):\n",
        "    variance = jnp.mean(jnp.square(x), axis=-1, keepdims=True)\n",
        "    return x * weight * jnp.reciprocal(jnp.sqrt(variance + eps))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Rotary Positional Encoding\n",
        "\n",
        "Transformers don't naturally know the order of tokens, so we need to add some position info. In llama3 to solve this we have ROPE. It does this by “rotating” the query and key vectors based on their position.\n",
        "\n",
        "![img](images/rope.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhnoLjFnWWq7"
      },
      "outputs": [],
      "source": [
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (jnp.arange(0, dim // 2, dtype=jnp.float32) / dim))\n",
        "    t = jnp.arange(end, dtype=jnp.float32)\n",
        "    freqs = jnp.outer(t, freqs)\n",
        "    return jnp.complex64(jnp.exp(1j * freqs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How It Works:\n",
        "\n",
        "Precompute Rotation Factors: First we create a table of rotation factors using a range of frequencies. This means each token gets its own unique rotation angle.\n",
        "\n",
        "Apply the Rotation:\n",
        "\n",
        "Pair Up Features: we reshape the vectors so that every two numbers form a pair; imagine them as the real and imaginary parts of a complex number.\n",
        "\n",
        "Rotate: We multiply these complex numbers by our precomputed rotation factors. This rotates each pair in the complex plane.\n",
        "\n",
        "Convert Back: Finally, we split the rotated complex numbers back into their real and imaginary parts to restore the original shape.\n",
        "\n",
        "\n",
        "##### Math Behind It:\n",
        "\n",
        "For each pair $(x_{2i}, x_{2i+1})$, the rotation is given by:\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "x'_{2i} \\\\\n",
        "x'_{2i+1}\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "\\cos(\\theta_i) & -\\sin(\\theta_i) \\\\\n",
        "\\sin(\\theta_i) & \\cos(\\theta_i)\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "x_{2i} \\\\\n",
        "x_{2i+1}\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "where $\\theta_i$ is the rotation angle for that token.\n",
        "In short, ROPE embeds positional information directly into the token features by rotating them. This way attention module gets the info about token order without extra position vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fUQVRriWWoU"
      },
      "outputs": [],
      "source": [
        "def apply_rotary_emb(xq, xk, freqs_cis):\n",
        "    xq_r, xk_r = jnp.reshape(xq, (*xq.shape[:-1], -1, 2)), jnp.reshape(xk, (*xk.shape[:-1], -1, 2))\n",
        "    xq_complex = jnp.complex64(xq_r[..., 0] + 1j * xq_r[..., 1])\n",
        "    xk_complex = jnp.complex64(xk_r[..., 0] + 1j * xk_r[..., 1])\n",
        "    freqs_cis = jnp.reshape(freqs_cis, (1, freqs_cis.shape[0], 1, freqs_cis.shape[1]))\n",
        "    xq_out = xq_complex * freqs_cis\n",
        "    xk_out = xk_complex * freqs_cis\n",
        "    xq = jnp.stack([jnp.real(xq_out), jnp.imag(xq_out)], axis=-1).reshape(xq.shape)\n",
        "    xk = jnp.stack([jnp.real(xk_out), jnp.imag(xk_out)], axis=-1).reshape(xk.shape)\n",
        "    return xq, xk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HshjmBZUWWlu"
      },
      "outputs": [],
      "source": [
        "def repeat_kv(x, n_rep):\n",
        "    return x if n_rep == 1 else jnp.repeat(x, n_rep, axis=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Weights Initialization\n",
        "\n",
        "In pure JAX, we don't use classes like in PyTorch. We use only pure fucntions why ? cause it makes our code more predictable and easier to parallelize. A pure function always returns the same output for the same input and doesn’t cause any side effects.6 For example, if you call F(x), you'll always get the same y.\n",
        "\n",
        "Since we aren’t using a framework like PyTorch’s nn.Module to automatically track parameters, we must initialize and update our weights manually.\n",
        "\n",
        "Handling randomness is also different. Instead of relying on a single global seed as in NumPy or PyTorch, in jax we need to manage randomness with explicit pseudo-random number generator (PRNG) keys. Each random operation gets its own unique key, which is derived by splitting a parent key. This will help in reproducibility and parallelism.\n",
        "\n",
        "For example, below you can see we are creating a key and splitting it into sub keys and then providing that key to the function which involves the randomness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now lets start with our Model Weights Initialization, first we create the random values for our parameters with normal ditribuition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uF3_5uR-WWi2"
      },
      "outputs": [],
      "source": [
        "def init_weight(key, shape, scale=None):\n",
        "    scale = 1.0 / math.sqrt(shape[0]) if scale is None else scale\n",
        "    return jax.random.normal(key, shape) * scale\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we'll identify all the learnable parameters of our model(llama3), assign each a unique key to ensure reproducibility, and apply the initialization process to them.\n",
        "\n",
        "Since weights are essentially numbers stored in arrays, we can use dictionaries to manage them as key-value pairs.\n",
        "\n",
        "First we will start with attention module which has four trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_LCmWEYWWf_"
      },
      "outputs": [],
      "source": [
        "def init_attention_weights(key, dim, n_heads, n_kv_heads):\n",
        "    keys = jax.random.split(key, 4)\n",
        "    head_dim = dim // n_heads\n",
        "    return {\n",
        "        'wq': init_weight(keys[0], (dim, n_heads * head_dim)),\n",
        "        'wk': init_weight(keys[1], (dim, n_kv_heads * head_dim)),\n",
        "        'wv': init_weight(keys[2], (dim, n_kv_heads * head_dim)),\n",
        "        'wo': init_weight(keys[3], (n_heads * head_dim, dim))\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next we have our Feed-forward network which has 3 trainable parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_ffn_weights(key, dim):\n",
        "    keys = jax.random.split(key, 3)\n",
        "    return {\n",
        "        'w1': init_weight(keys[0], (dim, 4 * dim)),\n",
        "        'w2': init_weight(keys[1], (4 * dim, dim)),\n",
        "        'w3': init_weight(keys[2], (dim, 4 * dim))}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we combine our weights into transformer block, adding two additional parameters for two layers of RMSNorm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1tN-AaCWWdp"
      },
      "outputs": [],
      "source": [
        "def init_transformer_block(key, dim, n_heads, n_kv_heads):\n",
        "    keys = jax.random.split(key, 4)\n",
        "    return {\n",
        "        'attention': init_attention_weights(keys[0], dim, n_heads, n_kv_heads),\n",
        "        'ffn': init_ffn_weights(keys[1], dim),\n",
        "        'attention_norm': init_weight(keys[2], (dim,), scale=1.0),\n",
        "        'ffn_norm': init_weight(keys[3], (dim,), scale=1.0)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we assemble Model's Weights Initialization in one place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def init_model_params(key, vocab_size, dim, n_layers, n_heads, n_kv_heads):\n",
        "    keys = jax.random.split(key, 4)\n",
        "    params = {\n",
        "        'token_embedding': init_weight(keys[0], (vocab_size, dim)),\n",
        "        'norm_f': init_weight(keys[1], (dim,), scale=1.0),\n",
        "        'output': init_weight(keys[2], (dim, vocab_size))\n",
        "    }\n",
        "    block_keys = jax.random.split(keys[3], n_layers)\n",
        "    params['blocks'] = [init_transformer_block(k, dim, n_heads, n_kv_heads) for k in block_keys]\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Group-Query Attention\n",
        "\n",
        "Now it's time for attention. Grouped Query Attention (GQA) is an optimized version of Multi-Head Attention that improves efficiency by sharing key and value representations among multiple query heads. This reduces computational overhead and memory usage, enabling faster inference and better scaling for transformer models. At it's core, it's just self-attention but with some modification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O349zNnrWqeG"
      },
      "outputs": [],
      "source": [
        "def attention(params, x, mask, freqs_cis, n_heads, n_kv_heads, cache=None, position=0):\n",
        "    B, T, C = x.shape\n",
        "    head_dim = C // n_heads\n",
        "    q = jnp.dot(x, params['wq']).reshape(B, T, n_heads, head_dim)\n",
        "    k = jnp.dot(x, params['wk']).reshape(B, T, n_kv_heads, head_dim)\n",
        "    v = jnp.dot(x, params['wv']).reshape(B, T, n_kv_heads, head_dim)\n",
        "    q, k = apply_rotary_emb(q, k, freqs_cis[position:position + T])\n",
        "    if cache is not None:\n",
        "        k = jnp.concatenate([cache[0], k], axis=1)\n",
        "        v = jnp.concatenate([cache[1], v], axis=1)\n",
        "    new_cache = (k, v)\n",
        "    k = repeat_kv(k, n_heads // n_kv_heads)\n",
        "    v = repeat_kv(v, n_heads // n_kv_heads)\n",
        "    q, k, v = map(lambda x: x.transpose(0, 2, 1, 3), (q, k, v))\n",
        "    scores = jnp.matmul(q, k.transpose(0, 1, 3, 2)) / math.sqrt(head_dim)\n",
        "    if mask is not None:\n",
        "        scores = scores + mask[:, :, :T, :T]\n",
        "    scores = jax.nn.softmax(scores, axis=-1)\n",
        "    output = jnp.matmul(scores, v)\n",
        "    output = output.transpose(0, 2, 1, 3).reshape(B, T, -1)\n",
        "    return jnp.dot(output, params['wo']), new_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### KV-cache :\n",
        "\n",
        "It stores previously computed key (K) and value (V) tensors from past tokens. We can cache this kv-cache during inference.\n",
        "\n",
        "\n",
        "![img](images/lightkv.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feed-forward\n",
        "\n",
        "This is simple feed-forward network with SiLU activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwijVToGWqbg"
      },
      "outputs": [],
      "source": [
        "def feed_forward(params, x):\n",
        "    return jnp.dot(jax.nn.silu(jnp.dot(x, params['w3'])) * jnp.dot(x, params['w1']), params['w2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transformer-block\n",
        "\n",
        "This is where all the important components come together in the transformer block. We unpack the pre-initialized weights and distribute them to their respective layers. The transformer blocks include attention, normalization, feed-forward processing layers and residual connections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOvB3nH2WqYY"
      },
      "outputs": [],
      "source": [
        "def transformer_block(params, x, mask, freqs_cis, n_heads, n_kv_heads, cache=None, position=0, training=False, dropout_rate=0.0, key=None):\n",
        "    attn_output, new_cache = attention(params['attention'], rms_norm(x, params['attention_norm']), mask, freqs_cis, n_heads, n_kv_heads, cache, position)\n",
        "    if training:\n",
        "        dropout_key, key = jax.random.split(key)\n",
        "        attn_output = jax.random.bernoulli(dropout_key, 1-dropout_rate, shape=attn_output.shape) * attn_output / (1-dropout_rate)\n",
        "    h = x + attn_output\n",
        "    ffn_output = feed_forward(params['ffn'], rms_norm(h, params['ffn_norm']))\n",
        "    if training:\n",
        "        dropout_key, key = jax.random.split(key)\n",
        "        ffn_output = jax.random.bernoulli(dropout_key, 1-dropout_rate, shape=ffn_output.shape) * ffn_output / (1-dropout_rate)\n",
        "    out = h + ffn_output\n",
        "    return out, new_cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Forward-Pass\n",
        "\n",
        "The forward pass takes your data through the entire model from converting input tokens into embeddings, through a series of transformer blocks, and finally to the output layer. In other words, it connects all the layers (embedding, transformers, and output) to produce the final predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBsGIOvUWqVg"
      },
      "outputs": [],
      "source": [
        "def model_forward(params, inputs, config, cache=None, position=0):\n",
        "    B, T = inputs.shape\n",
        "    h = params['token_embedding'][inputs]\n",
        "    freqs_cis = precompute_freqs_cis(config.dim // config.n_heads, config.max_seq_len)\n",
        "    mask = jnp.tril(jnp.ones((config.max_seq_len, config.max_seq_len)))\n",
        "    mask = jnp.where(mask == 0, -1e9, 0.0)\n",
        "    mask = mask.astype(h.dtype)\n",
        "    mask = mask[None, None, :, :]\n",
        "    new_caches = []\n",
        "    for i, block in enumerate(params['blocks']):\n",
        "        layer_cache = cache[i] if cache is not None else None\n",
        "        h, layer_cache = transformer_block(block, h, mask, freqs_cis, config.n_heads, config.n_kv_heads, layer_cache, position, training=False, dropout_rate=config.dropout_rate)\n",
        "        new_caches.append(layer_cache)\n",
        "    h = rms_norm(h, params['norm_f'])\n",
        "    logits = jnp.dot(h, params['output'])\n",
        "    return logits, new_caches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-4FpWPkWyIO"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5uol2zBWqSa",
        "outputId": "b562b1f4-f37b-4329-a02e-4af66aa91a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kAtOa113WWay"
      },
      "outputs": [],
      "source": [
        "from jax import random, vmap\n",
        "import tiktoken\n",
        "from functools import partial\n",
        "import os\n",
        "import jax.lax as lax\n",
        "import pickle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSVQQ3spW9FL"
      },
      "outputs": [],
      "source": [
        "os.environ['JAX_PLATFORM_NAME'] = 'tpu' # gpu or tpu \n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false' # this will prevent jax from preallocsting 75% vram.\n",
        "print(\"JAX devices:\", jax.devices())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization\n",
        "\n",
        "Tokenization means dividing the text into words and subwords (tokens). We will be using Byte Pair Encoding (BPE) for training our model (BPE was used in training Llama 3).7 I will not build bpe from scratch we will use tiktoken library by openai for bpe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Le2AlBOfW_pD"
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer and load data\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "with open('shakespeare.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "tokens = enc.encode(text)\n",
        "data = jnp.array(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Config \n",
        "\n",
        " So these are the hyperparameter we need to train approximately 2 million parameters model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UOpLvfrXCzI"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "class ModelConfig:\n",
        "    vocab_size = enc.n_vocab\n",
        "    dim = 256\n",
        "    n_layers = 6\n",
        "    n_heads = 8\n",
        "    n_kv_heads = 4\n",
        "    max_seq_len = 512\n",
        "    batch_size = 32\n",
        "    learning_rate = 3e-4\n",
        "    dropout_rate = 0.0\n",
        "\n",
        "config = ModelConfig()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp6QB1DpXCwQ"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "key = random.PRNGKey(0)\n",
        "params = init_model_params(\n",
        "    key=key,\n",
        "    vocab_size=config.vocab_size,\n",
        "    dim=config.dim,\n",
        "    n_layers=config.n_layers,\n",
        "    n_heads=config.n_heads,\n",
        "    n_kv_heads=config.n_kv_heads\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### save and load model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgrlnqknXFtW"
      },
      "outputs": [],
      "source": [
        "def save_params(params, filepath):\n",
        "    numpy_params = jax.tree.map(lambda x: x.copy(), params)\n",
        "    with open(filepath, 'wb') as f:\n",
        "        pickle.dump(numpy_params, f)\n",
        "\n",
        "def load_params(filepath):\n",
        "    with open(filepath, 'rb') as f:\n",
        "        numpy_params = pickle.load(f)\n",
        "    # convert back to JAX arrays\n",
        "    params = jax.tree.map(lambda x: jnp.array(x), numpy_params)\n",
        "    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get Batches\n",
        "\n",
        "The get_batch function creates training batches from our Shakespeare dataset. We need to feed our model with chunks of data. For each batch, we randomly select starting positions in the text, this way the model sees a variety of contexts.\n",
        "\n",
        "Now, here's where JAX's cool vmap feature comes into play. Instead of writing a loop to extract each chunk, we use vmap to automate.\n",
        "\n",
        "How does it work ?\n",
        "\n",
        "vmap is like a vectorized loop; it takes a function that processes a single index (using lax.dynamic_slice to get a sequence of tokens) and applies it to every element in our array of indices. This means our input sequences (x) and corresponding target sequences (y, which are shifted by one token for next-word prediction) are created in one go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z21vUsy-XFqt"
      },
      "outputs": [],
      "source": [
        "def get_batch(key, data, batch_size, seq_len):\n",
        "    # Generate random starting indices\n",
        "    ix = random.randint(key, (batch_size,), 0, len(data) - seq_len)\n",
        "\n",
        "    # Vectorized operation to get input and target sequences\n",
        "    x = vmap(lambda i: lax.dynamic_slice(data, (i,), (seq_len,)))(ix)\n",
        "    y = vmap(lambda i: lax.dynamic_slice(data, (i + 1,), (seq_len,)))(ix)\n",
        "\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g06bZfwnXFoH"
      },
      "outputs": [],
      "source": [
        "def generate(params, prompt_tokens, max_new_tokens, config):\n",
        "    x = jnp.array(prompt_tokens)\n",
        "    for _ in range(max_new_tokens):\n",
        "        x_crop = x[-config.max_seq_len:]\n",
        "        logits, _ = model_forward(params, x_crop[None, :], config)\n",
        "        logits = logits[0, -1, :]  # take the last logit\n",
        "        next_token = random.categorical(random.PRNGKey(0), logits, shape=(1,))[0]\n",
        "        x = jnp.concatenate([x, jnp.array([next_token])])\n",
        "    return x.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss function\n",
        "\n",
        "This function computes the cross-entropy loss for a batch during training. It first performs a forward pass using the model to generate logits for the input data. Then, it reshapes both the logits and targets to merge the batch and sequence dimensions. After applying the log softmax to the logits, it extracts the log probabilities corresponding to the correct target tokens and computes their negative mean as the final loss value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCheBH9PXFlP"
      },
      "outputs": [],
      "source": [
        "def compute_loss(params, batch):\n",
        "    inputs, targets = batch\n",
        "    logits, _ = model_forward(params, inputs, config)\n",
        "    logits = logits.reshape(-1, config.vocab_size)\n",
        "    targets = targets.reshape(-1)\n",
        "    loss = -jnp.mean(\n",
        "        jnp.take_along_axis(\n",
        "            jax.nn.log_softmax(logits),\n",
        "            targets[:, None],\n",
        "            axis=1\n",
        "        )\n",
        "    )\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Update function\n",
        "\n",
        "Now we need to write a function to update our weights. For simplicity, we're using Stochastic Gradient Descent (SGD) here, though you can also use Adam or AdamW for faster convergence.\n",
        "\n",
        "In the code, you'll notice the @jax.jit decorator. This is one of the features that sets jax apart. JIT (Just-In-Time) compilation speeds up execution by converting your Python code into optimized machine code.\n",
        "\n",
        "How does it work ?\n",
        "\n",
        "When you decorate a function with JAX’s jit, it changes how the function executes. Normally, when you call a function, Python runs it line by line. For example, if you have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HI jiited\n",
            "4\n",
            "HI jiited\n",
            "9\n",
            "HI jiited\n",
            "16\n"
          ]
        }
      ],
      "source": [
        "def sqr(x): \n",
        "    print(\"HI jiited\") # side effect \n",
        "    return x * x\n",
        "\n",
        "print(sqr(2)) \n",
        "print(sqr(3)) \n",
        "print(sqr(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HI jiited\n",
            "4\n",
            "9\n",
            "16\n"
          ]
        }
      ],
      "source": [
        "@jax.jit\n",
        "def sqr(x): \n",
        "    print(\"HI jiited\") # side effect  \n",
        "    return x * x\n",
        "\n",
        "print(sqr(2)) \n",
        "print(sqr(3)) \n",
        "print(sqr(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Jax first traces your function to build an optimized computation graph. This tracing happens the first time the function is called and converts the Python code into machine code.\n",
        "\n",
        "Because of this tracing, any side effects like the print statement; are only executed during the initial tracing. Once the function is compiled, other remaining calls use the compiled version, and you might not see the print output every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNul4U7tXFic"
      },
      "outputs": [],
      "source": [
        "@jax.jit\n",
        "def update_step(params, batch):\n",
        "    loss, grads = jax.value_and_grad(compute_loss)(params, batch)\n",
        "    params = jax.tree.map(\n",
        "        lambda p, g: p - config.learning_rate * g,\n",
        "        params,\n",
        "        grads\n",
        "    )\n",
        "    return params, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In our update_step function, @jax.jit compiles the code. The function computes loss and gradients simultaneously with jax.value_and_grad, updates the parameters using gradient descent with help of jax.tree.map, and returns the updated parameters and loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Trainig-Loop\n",
        "\n",
        "Finally, its time to train our 2 million parameter model on shakespeare dataset. We first prepare batches using the get_batch which splits our data into batches so we can train faster with our limited compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TpE30iJXCta"
      },
      "outputs": [],
      "source": [
        "def train(num_epochs=30, steps_per_epoch=1000):\n",
        "    key = random.PRNGKey(0)\n",
        "    params_state = params  # copying\n",
        "\n",
        "    epoch_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        epoch_loss = 0.0\n",
        "        for step in range(steps_per_epoch):\n",
        "\n",
        "            key, batch_key = random.split(key)\n",
        "\n",
        "            # Get batch\n",
        "            batch = get_batch(batch_key, data, config.batch_size, config.max_seq_len)\n",
        "\n",
        "            # Update model\n",
        "            params_state, loss = update_step(params_state, batch)\n",
        "            epoch_loss += loss\n",
        "\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                print(f\"epoch {epoch + 1}, step {step}/{steps_per_epoch}: loss = {loss:.4f}\")\n",
        "\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / steps_per_epoch\n",
        "        epoch_losses.append(avg_epoch_loss)\n",
        "\n",
        "        print(f\"\\nepoch {epoch + 1} | average loss: {avg_epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            save_params(params_state, f'model_checkpoint_epoch_{epoch+1}.pkl')\n",
        "\n",
        "\n",
        "    print(\"Loss by epoch:\")\n",
        "    for epoch, loss in enumerate(epoch_losses, 1):\n",
        "        print(f\"Epoch {epoch}: {loss:.4f}\")\n",
        "\n",
        "    # Save final model\n",
        "    save_params(params_state, 'model_final.pkl')\n",
        "    return params_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-jenVeGXO6H"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trained_params = train()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
